# Incident Response Framework

This directory contains a comprehensive incident response framework for SLURM Exporter production deployments, providing structured procedures, automated response capabilities, and detailed playbooks for handling various incident scenarios.

## Overview

The incident response framework provides:
- **Incident Classification**: Standardized severity levels and response times
- **Automated Detection**: Prometheus-based alerting and incident detection
- **Automated Response**: Self-healing capabilities for common incidents  
- **Detailed Playbooks**: Step-by-step procedures for critical scenarios
- **Communication Templates**: Standardized incident communication
- **Post-Incident Process**: Lessons learned and continuous improvement

## Framework Components

### 1. Incident Types (`incident-types.yaml`)

#### Security Incidents
- **Security Breach**: Unauthorized access or data breach (Critical, 15 min response)
- **Vulnerability Exploit**: Active exploitation attempts (Critical, 30 min response)
- **Malware Detection**: Malicious software in containers (Critical, 15 min response)
- **Compliance Violation**: Policy violations (High, 2 hour response)

#### Operational Incidents
- **Service Outage**: Complete unavailability (Critical, 15 min response)
- **Performance Degradation**: Significant slowdown (High, 1 hour response)
- **Data Loss**: Missing metrics or configuration (High, 30 min response)
- **API Failure**: SLURM connectivity issues (Medium, 2 hour response)

#### Infrastructure Incidents
- **Cluster Failure**: Kubernetes issues (Critical, 30 min response)
- **Resource Exhaustion**: CPU/Memory limits (High, 1 hour response)
- **Network Partition**: Connectivity issues (High, 1 hour response)

### 2. Automated Response (`automated-response.yaml`)

#### Incident Response Controller
- Continuous monitoring for incident conditions
- Automatic detection of security anomalies
- Self-healing for common operational issues
- Evidence collection and preservation

#### Automated Actions
```yaml
security_anomaly:
  - Collect security logs
  - Apply network restrictions
  - Notify security team

service_outage:
  - Capture diagnostics
  - Attempt restart
  - Rollback if needed

performance_degradation:
  - Collect metrics
  - Increase resources
  - Notify operations team
```

### 3. Incident Playbooks (`playbooks.yaml`)

#### Available Playbooks
1. **Security Breach Response**: Complete security incident handling
2. **Service Degradation**: Performance issue resolution
3. **Disaster Recovery**: Complete service restoration

## Usage Instructions

### Initial Setup

#### 1. Deploy Incident Response Framework
```bash
# Deploy all incident response components
kubectl apply -f incident-types.yaml
kubectl apply -f automated-response.yaml
kubectl apply -f playbooks.yaml

# Create notification secrets
kubectl create secret generic incident-notifications \
  --from-literal=slack-webhook="https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK" \
  --from-literal=pagerduty-token="YOUR_PAGERDUTY_TOKEN" \
  -n slurm-exporter
```

#### 2. Configure AlertManager Integration
```yaml
# alertmanager-config.yaml
route:
  receiver: 'slurm-exporter-incidents'
  routes:
  - match:
      alertname: SecurityIncidentDetected
    receiver: 'security-critical'
    continue: true
  - match:
      component: incident-response
    receiver: 'incident-webhook'

receivers:
- name: 'incident-webhook'
  webhook_configs:
  - url: 'http://incident-webhook.slurm-exporter:10341/webhook'
    send_resolved: true
```

#### 3. Verify Setup
```bash
# Check incident response controller
kubectl get deployment incident-response-controller -n slurm-exporter
kubectl logs deployment/incident-response-controller -n slurm-exporter

# Verify webhook receiver
kubectl get service incident-webhook -n slurm-exporter

# Test incident detection
kubectl get prometheusrule incident-detection-rules -n slurm-exporter
```

### Incident Response Process

#### 1. Incident Detection
```bash
# Monitor active incidents
kubectl logs deployment/incident-response-controller -n slurm-exporter -f

# Check Prometheus alerts
kubectl exec prometheus-0 -n monitoring -- \
  promtool query instant 'ALERTS{component="incident-response"}'

# Review recent incidents
ls -la /var/incident-response/incident-*.json
```

#### 2. Manual Incident Response
```bash
# Trigger manual incident response
kubectl exec deployment/incident-response-controller -n slurm-exporter -- \
  /scripts/respond.sh "service_outage" "critical"

# Execute specific playbook
kubectl create job incident-$(date +%s) -n slurm-exporter --from=- <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: incident-$(date +%s)
spec:
  template:
    spec:
      containers:
      - name: responder
        image: alpine:3.18
        command: ["/bin/sh", "-c", "echo 'Executing security breach playbook'"]
      restartPolicy: Never
EOF
```

#### 3. Incident Communication
```bash
# Send incident notification
curl -X POST "$SLACK_WEBHOOK" \
  -H 'Content-type: application/json' \
  -d '{
    "text": "ðŸš¨ Incident Alert",
    "attachments": [{
      "color": "danger",
      "fields": [
        {"title": "Type", "value": "Service Outage", "short": true},
        {"title": "Severity", "value": "Critical", "short": true},
        {"title": "Status", "value": "Investigating", "short": false}
      ]
    }]
  }'
```

### Playbook Execution

#### Security Breach Response
```bash
#!/bin/bash
# Execute security breach playbook

# Phase 1: Containment
kubectl scale deployment slurm-exporter --replicas=0 -n slurm-exporter
kubectl apply -f emergency-network-policy.yaml

# Phase 2: Investigation
./collect-evidence.sh
./analyze-logs.sh

# Phase 3: Eradication
kubectl delete secret slurm-credentials -n slurm-exporter
./rotate-credentials.sh

# Phase 4: Recovery
kubectl apply -f deployment/production/
kubectl scale deployment slurm-exporter --replicas=3 -n slurm-exporter

# Phase 5: Lessons Learned
./generate-incident-report.sh
```

#### Performance Degradation Response
```bash
#!/bin/bash
# Execute performance degradation playbook

# Phase 1: Assessment
kubectl top pods -n slurm-exporter
./check-dependencies.sh

# Phase 2: Mitigation
kubectl patch deployment slurm-exporter -n slurm-exporter \
  --patch '{"spec":{"template":{"spec":{"containers":[{"name":"slurm-exporter","resources":{"limits":{"cpu":"2","memory":"2Gi"}}}]}}}}'

# Phase 3: Root Cause Analysis
./analyze-metrics.sh
./profile-application.sh

# Phase 4: Optimization
kubectl apply -f optimized-config.yaml

# Phase 5: Validation
./run-performance-tests.sh
```

### Automated Response Configuration

#### Custom Response Rules
```yaml
# custom-responses.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-incident-responses
  namespace: slurm-exporter
data:
  responses.yaml: |
    custom_responses:
      high_error_rate:
        trigger:
          metric: "rate(slurm_exporter_errors_total[5m]) > 0.1"
          duration: "5m"
        actions:
          - restart_deployment
          - increase_logging
          - notify_team
      
      memory_leak:
        trigger:
          metric: "rate(container_memory_usage_bytes[1h]) > 0"
          duration: "30m"
        actions:
          - capture_heap_dump
          - restart_pod
          - create_ticket
```

#### Response Automation Script
```bash
#!/bin/bash
# Automated response executor

INCIDENT_TYPE="$1"
SEVERITY="$2"

case "$INCIDENT_TYPE" in
  "security_*")
    # Security response
    kubectl scale deployment slurm-exporter --replicas=0 -n slurm-exporter
    ./collect-security-evidence.sh
    ./notify-security-team.sh
    ;;
  
  "operational_*")
    # Operational response
    ./capture-diagnostics.sh
    kubectl rollout restart deployment/slurm-exporter -n slurm-exporter
    ./notify-ops-team.sh
    ;;
  
  "infrastructure_*")
    # Infrastructure response
    ./check-cluster-health.sh
    ./scale-resources.sh
    ./notify-platform-team.sh
    ;;
esac
```

## Incident Severity Levels

### Critical (P1)
- **Response Time**: 15 minutes
- **Notification**: Immediate page to on-call, security team, management
- **Examples**: Security breach, complete outage, data loss
- **Escalation**: Automatic after 30 minutes

### High (P2)
- **Response Time**: 1 hour
- **Notification**: On-call engineer, team lead
- **Examples**: Performance degradation, partial outage
- **Escalation**: After 2 hours

### Medium (P3)
- **Response Time**: 4 hours
- **Notification**: On-call engineer
- **Examples**: Non-critical errors, minor degradation
- **Escalation**: Next business day

### Low (P4)
- **Response Time**: 24 hours
- **Notification**: Team channel
- **Examples**: Cosmetic issues, minor bugs
- **Escalation**: Weekly review

## Communication Templates

### Initial Incident Notification
```
ðŸš¨ INCIDENT ALERT - [SEVERITY]

**Incident ID**: INC-YYYYMMDD-XXXX
**Type**: [Incident Type]
**Severity**: [Critical/High/Medium/Low]
**Status**: Investigating
**Impact**: [User impact description]

**Current Actions**:
- Incident response team engaged
- Initial investigation underway
- [Specific actions being taken]

**Next Update**: In 30 minutes
```

### Status Update Template
```
ðŸ“Š INCIDENT UPDATE - INC-YYYYMMDD-XXXX

**Status**: [Investigating/Identified/Monitoring/Resolved]
**Time Since Start**: [Duration]

**Progress**:
- [Completed action 1]
- [Completed action 2]
- [Current action]

**Next Steps**:
- [Planned action 1]
- [Planned action 2]

**ETA**: [Estimated resolution time]
**Next Update**: In [X] minutes
```

### Resolution Notification
```
âœ… INCIDENT RESOLVED - INC-YYYYMMDD-XXXX

**Duration**: [Total time]
**Root Cause**: [Brief description]
**Resolution**: [What was done]

**Impact Summary**:
- [Impact metric 1]
- [Impact metric 2]

**Follow-up Actions**:
- Post-incident review scheduled for [date/time]
- [Additional action items]

Thank you for your patience during this incident.
```

## Post-Incident Process

### Incident Review Meeting
1. **Schedule**: Within 48 hours of resolution
2. **Participants**: Response team, stakeholders, service owners
3. **Duration**: 60 minutes maximum
4. **Output**: Action items and process improvements

### Incident Report Template
```markdown
# Incident Report - INC-YYYYMMDD-XXXX

## Executive Summary
- **Incident Type**: 
- **Severity**: 
- **Duration**: 
- **User Impact**: 

## Timeline
- **HH:MM** - Incident detected
- **HH:MM** - Response initiated
- **HH:MM** - Root cause identified
- **HH:MM** - Resolution implemented
- **HH:MM** - Service restored

## Root Cause Analysis
### What Happened
[Detailed description]

### Why It Happened
[Root cause analysis]

### Contributing Factors
- [Factor 1]
- [Factor 2]

## Response Evaluation
### What Went Well
- [Positive aspect 1]
- [Positive aspect 2]

### What Could Be Improved
- [Improvement area 1]
- [Improvement area 2]

## Action Items
| Action | Owner | Due Date | Priority |
|--------|-------|----------|----------|
| [Action 1] | [Name] | [Date] | [P1/P2/P3] |
| [Action 2] | [Name] | [Date] | [P1/P2/P3] |

## Lessons Learned
- [Lesson 1]
- [Lesson 2]
```

### Continuous Improvement
1. **Monthly Review**: Analyze incident trends
2. **Quarterly Training**: Update team on procedures
3. **Annual Assessment**: Complete framework review
4. **Automation**: Identify manual tasks to automate

## Testing and Drills

### Monthly Incident Drills
```bash
#!/bin/bash
# Monthly incident response drill

# Scenario rotation
SCENARIOS=(
  "security_breach"
  "service_outage"
  "data_loss"
  "performance_degradation"
)

# Select random scenario
SCENARIO=${SCENARIOS[$RANDOM % ${#SCENARIOS[@]}]}

echo "Starting incident drill: $SCENARIO"

# Simulate incident
kubectl create job drill-$(date +%s) -n slurm-exporter --from=- <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: drill-$(date +%s)
  labels:
    incident-drill: "true"
spec:
  template:
    spec:
      containers:
      - name: drill
        image: alpine:3.18
        command: ["/bin/sh", "-c", "echo 'Simulating $SCENARIO incident'"]
      restartPolicy: Never
EOF

# Monitor response
./monitor-drill-response.sh
```

### Chaos Engineering
```yaml
# chaos-testing.yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: slurm-exporter-chaos
  namespace: slurm-exporter
spec:
  selector:
    namespaces:
    - slurm-exporter
    labelSelectors:
      app: slurm-exporter
  mode: one
  action: pod-kill
  duration: "60s"
  scheduler:
    cron: "@weekly"
```

## Integration with External Systems

### PagerDuty Integration
```yaml
# pagerduty-integration.yaml
incidents:
  routing_keys:
    critical: "YOUR_CRITICAL_ROUTING_KEY"
    high: "YOUR_HIGH_ROUTING_KEY"
    medium: "YOUR_MEDIUM_ROUTING_KEY"
  
  escalation_policies:
    security: "security-team-escalation"
    operational: "platform-team-escalation"
```

### Slack Integration
```yaml
# slack-integration.yaml
channels:
  incidents: "#incidents"
  security: "#security-incidents"
  operations: "#ops-incidents"
  
notifications:
  critical:
    mentions: ["@channel"]
    color: "danger"
  high:
    mentions: ["@oncall"]
    color: "warning"
```

This comprehensive incident response framework ensures rapid, consistent, and effective handling of all incident types, minimizing service impact and improving overall system reliability.