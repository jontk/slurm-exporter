{{- if .Values.prometheusRule.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "slurm-exporter.fullname" . }}
  namespace: {{ default .Release.Namespace .Values.prometheusRule.namespace | quote }}
  labels:
    {{- include "slurm-exporter.labels" . | nindent 4 }}
    {{- with .Values.prometheusRule.labels }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
  {{- with .Values.prometheusRule.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  groups:
  {{- if .Values.prometheusRule.rules }}
  {{- with .Values.prometheusRule.rules }}
  {{- toYaml . | nindent 2 }}
  {{- end }}
  {{- else }}
  - name: slurm-exporter.rules
    interval: 30s
    rules:
    # Exporter availability alerts
    - alert: SlurmExporterDown
      expr: up{job="slurm-exporter"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "SLURM Exporter is down"
        description: "SLURM Exporter has been down for more than 1 minute."

    - alert: SlurmExporterHighMemoryUsage
      expr: container_memory_usage_bytes{pod=~"{{ include "slurm-exporter.fullname" . }}.*"} / container_spec_memory_limit_bytes > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "SLURM Exporter high memory usage"
        description: "SLURM Exporter memory usage is above 80% for more than 5 minutes."

    - alert: SlurmExporterHighCPUUsage
      expr: rate(container_cpu_usage_seconds_total{pod=~"{{ include "slurm-exporter.fullname" . }}.*"}[5m]) / container_spec_cpu_quota * 100 > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "SLURM Exporter high CPU usage"
        description: "SLURM Exporter CPU usage is above 80% for more than 5 minutes."

    # SLURM cluster alerts
    - alert: SlurmAPIConnectionFailed
      expr: slurm_exporter_slurm_api_requests_total{status="error"} > 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "SLURM API connection failed"
        description: "Failed to connect to SLURM API for more than 2 minutes."

    - alert: SlurmClusterHighUtilization
      expr: slurm_cluster_utilization_percent > 90
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "SLURM cluster high utilization"
        description: "SLURM cluster utilization is above 90% for more than 10 minutes."

    - alert: SlurmNodesDown
      expr: slurm_nodes_total{state="down"} > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "SLURM nodes are down"
        description: "{{ $value }} SLURM nodes are in down state."

    - alert: SlurmJobsStuckInQueue
      expr: slurm_jobs_total{state="pending"} > 100
      for: 30m
      labels:
        severity: warning
      annotations:
        summary: "High number of jobs stuck in queue"
        description: "{{ $value }} jobs have been pending for more than 30 minutes."
  {{- end }}
{{- end }}