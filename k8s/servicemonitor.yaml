---
# ServiceMonitor for Prometheus Operator
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: slurm-exporter
  namespace: monitoring
  labels:
    app: slurm-exporter
    component: metrics
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: slurm-exporter
      service: metrics-endpoint
  endpoints:
  - port: http-metrics
    interval: 30s
    path: /metrics
    scheme: http
    scrapeTimeout: 10s
    honorLabels: true
    # TLS configuration (if TLS is enabled)
    # tlsConfig:
    #   insecureSkipVerify: true
    # Basic auth configuration (if basic auth is enabled)
    # basicAuth:
    #   username:
    #     name: slurm-exporter-basic-auth
    #     key: username
    #   password:
    #     name: slurm-exporter-basic-auth
    #     key: password
  namespaceSelector:
    matchNames:
    - monitoring
  targetLabels:
  - app
  - component
  - version

---
# PrometheusRule for SLURM exporter alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: slurm-exporter-alerts
  namespace: monitoring
  labels:
    app: slurm-exporter
    component: alerting
    prometheus: kube-prometheus
spec:
  groups:
  - name: slurm-exporter.rules
    interval: 30s
    rules:
    # Exporter availability alerts
    - alert: SlurmExporterDown
      expr: up{job="slurm-exporter"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "SLURM Exporter is down"
        description: "SLURM Exporter has been down for more than 1 minute."
    
    - alert: SlurmExporterHighMemoryUsage
      expr: container_memory_usage_bytes{pod=~"slurm-exporter.*"} / container_spec_memory_limit_bytes > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "SLURM Exporter high memory usage"
        description: "SLURM Exporter memory usage is above 80% for more than 5 minutes."
    
    - alert: SlurmExporterHighCPUUsage
      expr: rate(container_cpu_usage_seconds_total{pod=~"slurm-exporter.*"}[5m]) / container_spec_cpu_quota * 100 > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "SLURM Exporter high CPU usage"
        description: "SLURM Exporter CPU usage is above 80% for more than 5 minutes."
    
    # SLURM cluster alerts
    - alert: SlurmAPIConnectionFailed
      expr: slurm_exporter_slurm_api_requests_total{status="error"} > 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "SLURM API connection failed"
        description: "Failed to connect to SLURM API for more than 2 minutes."
    
    - alert: SlurmClusterHighUtilization
      expr: slurm_cluster_utilization_percent > 90
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "SLURM cluster high utilization"
        description: "SLURM cluster utilization is above 90% for more than 10 minutes."
    
    - alert: SlurmNodesDown
      expr: slurm_nodes_total{state="down"} > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "SLURM nodes are down"
        description: "{{ $value }} SLURM nodes are in down state."
    
    - alert: SlurmJobsStuckInQueue
      expr: slurm_jobs_total{state="pending"} > 100
      for: 30m
      labels:
        severity: warning
      annotations:
        summary: "High number of jobs stuck in queue"
        description: "{{ $value }} jobs have been pending for more than 30 minutes."